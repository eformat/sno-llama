---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: migrated-gpu
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/template-display-name: LLamaCPP
    opendatahub.io/template-name: llama-cpp
    openshift.io/display-name: sno-llama31-cpp
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    argocd.argoproj.io/sync-wave: "1"
    argocd.argoproj.io/compare-options: IgnoreExtraneous
  labels:
    opendatahub.io/dashboard: "true"
  name: sno-llama31-cpp
  namespace: llama-serving
spec:
  builtInAdapter:
    modelLoadingTimeoutMillis: 600000
  containers:
  - env:
    - name: LLAMA_ARG_MODEL
      value: /mnt/models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf
    - name: LLAMA_ARG_N_GPU_LAYERS
      value: "99999"
    - name: LLAMA_ARG_CTX_SIZE
      value: "32768" # "71680"
    - name: LLAMA_ARG_N_PARALLEL
      value: "3"
    - name: LLAMA_ARG_ENDPOINT_METRICS
      value: "1"
    - name: LLAMA_ARG_PORT
      value: "8080"
    - name: LLAMA_ARG_CHAT_TEMPLATE
      value: llama3
    image: quay.io/eformat/llama-cpp:latest
    name: kserve-container
    ports:
    - containerPort: 8080
      protocol: TCP
    resources:
      requests:
        cpu: 300m
        memory: 500Mi
    volumeMounts:
    - mountPath: /dev/shm
      name: shm
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: gguf
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 2Gi
    name: shm
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: sno-llama31-cpp
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    argocd.argoproj.io/sync-wave: "2"
    argocd.argoproj.io/compare-options: IgnoreExtraneous
  labels:
    opendatahub.io/dashboard: "true"
  name: sno-llama31-cpp
  namespace: llama-serving
spec:
  predictor:
    minReplicas: 1
    model:
      modelFormat:
        name: gguf
      name: ""
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
      runtime: sno-llama31-cpp
      storage:
        key: connection-minio
        path: Meta-Llama-3.1-8B-Instruct-Q8_0.gguf
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
---
kind: Secret
apiVersion: v1
metadata:
  name: connection-minio
  namespace: llama-serving
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/managed: 'true'
  annotations:
    opendatahub.io/connection-type: s3
    openshift.io/display-name: minio
data:
  AWS_ACCESS_KEY_ID: bWluaW8=
  AWS_DEFAULT_REGION: dXMtZWFzdC0x
  AWS_S3_BUCKET: bW9kZWxz
  AWS_S3_ENDPOINT: aHR0cDovL21pbmlvLm1pbmlvLnN2Yy5jbHVzdGVyLmxvY2FsOjkwMDA=
  AWS_SECRET_ACCESS_KEY: bWluaW8xMjM0
type: Opaque
